{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caq13/miniconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/caq13/miniconda3/envs/torch/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_generation_callback'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/caq13/realhome/CMCC/RODDA_SANI_VAE_PT.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bese-datalearning.ese.ic.ac.uk/home/caq13/realhome/CMCC/RODDA_SANI_VAE_PT.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bese-datalearning.ese.ic.ac.uk/home/caq13/realhome/CMCC/RODDA_SANI_VAE_PT.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39margparse\u001b[39;00m \u001b[39mimport\u001b[39;00m ArgumentParser\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bese-datalearning.ese.ic.ac.uk/home/caq13/realhome/CMCC/RODDA_SANI_VAE_PT.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata_generation_callback\u001b[39;00m \u001b[39mimport\u001b[39;00m MeshSampler\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bese-datalearning.ese.ic.ac.uk/home/caq13/realhome/CMCC/RODDA_SANI_VAE_PT.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_generation_callback'"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(42)\n",
    "from torch import nn\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "from data_generation_callback import MeshSampler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim = 64, enc_out_dim=64, latent_dim=32, input_dim=90351, output_dim=90351):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # encoder, decoder\n",
    "        self.encoder = nn.Sequential(nn.Linear(in_features = input_dim, out_features = hidden_dim),\n",
    "                                    nn.Sigmoid(),\n",
    "                                    nn.Linear(in_features = hidden_dim, out_features = enc_out_dim))\n",
    "\n",
    "        self.decoder = nn.Sequential(nn.Linear(in_features = latent_dim, out_features = hidden_dim),\n",
    "                                    nn.Sigmoid(),\n",
    "                                    nn.Linear(in_features = hidden_dim, out_features = output_dim),\n",
    "                                    nn.Sigmoid())\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(enc_out_dim, latent_dim)\n",
    "\n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def gaussian_likelihood(self, x_hat, logscale, x):\n",
    "        scale = torch.exp(logscale)\n",
    "        mean = x_hat\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "        # measure prob of seeing image under p(x|z)\n",
    "        log_pxz = dist.log_prob(x)\n",
    "        return log_pxz\n",
    "\n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "\n",
    "        # decoded\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "        recon_loss = ((x_hat - x)**2).mean()\n",
    "        # kl\n",
    "        kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "        # elbo\n",
    "        elbo = (0.0001*kl + recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        self.log_dict({\n",
    "            'elbo': elbo,\n",
    "            'kl': kl.mean(),\n",
    "            'recon_loss': recon_loss.mean(),\n",
    "            'reconstruction': recon_loss.mean(),\n",
    "            'kl': kl.mean(),\n",
    "        })\n",
    "\n",
    "        return elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 59/59 [00:19<00:00,  3.03it/s, loss=-0.436, v_num=106]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 217 M \n",
      "1 | decoder | Sequential | 221 M \n",
      "2 | fc_mu   | Linear     | 2.1 K \n",
      "3 | fc_var  | Linear     | 2.1 K \n",
      "---------------------------------------\n",
      "439 M     Trainable params\n",
      "0         Non-trainable params\n",
      "439 M     Total params\n",
      "1,756.714 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:   2%|▏         | 1/59 [00:00<00:00, 136.22it/s, loss=0.000369, v_num=107]"
     ]
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--gpus', type=int, default=0)\n",
    "parser.add_argument('--dataset', type=str, default='wavesuite')\n",
    "args, unknown = parser.parse_known_args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if args.dataset == 'CMCC':\n",
    "    directory = '/data/c-quilo/CMCC/'\n",
    "    input_filename = 'sateNo4_1_Unut_data_40_to_99.npy'\n",
    "    #input_filename = 'sateNo4_1_pcs_nut_data_40_to_99.npy'\n",
    "    input = np.load(directory + input_filename)\n",
    "\n",
    "    def scaler(x, xmin, xmax, min, max):\n",
    "        scale = (max - min) / (xmax - xmin)\n",
    "        xScaled = scale * x + min - xmin * scale\n",
    "        return xScaled\n",
    "\n",
    "    min_input = np.min(input)\n",
    "    max_input = np.max(input)\n",
    "\n",
    "    input = scaler(input, min_input, max_input, 0, 1)\n",
    "    input = torch.FloatTensor(input).to(device)\n",
    "\n",
    "sampler = MeshSampler()\n",
    "\n",
    "vae = VAE().to(device)\n",
    "trainer = pl.Trainer(gpus=args.gpus, max_epochs=500, accelerator='gpu')#, callbacks=[sampler])\n",
    "trainer.fit(vae, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6563, 0.6560, 0.6638,  ..., 0.6914, 0.6696, 0.6711],\n",
      "        [0.6525, 0.6909, 0.6404,  ..., 0.6707, 0.6656, 0.6713],\n",
      "        [0.6687, 0.6927, 0.6811,  ..., 0.6827, 0.6629, 0.7013],\n",
      "        ...,\n",
      "        [0.6626, 0.6656, 0.6791,  ..., 0.7010, 0.6583, 0.6940],\n",
      "        [0.7209, 0.6860, 0.7075,  ..., 0.6922, 0.6943, 0.7021],\n",
      "        [0.7070, 0.7047, 0.7090,  ..., 0.6997, 0.6879, 0.6684]])\n",
      "tensor([[-2.9694, -3.0044, -2.1723,  ...,  0.7819, -1.5543, -1.3917],\n",
      "        [-3.3781,  0.7209, -4.6683,  ..., -1.4345, -1.9753, -1.3664],\n",
      "        [-1.6506,  0.9180, -0.3241,  ..., -0.1512, -2.2625,  1.8302],\n",
      "        ...,\n",
      "        [-2.2920, -1.9801, -0.5355,  ...,  1.8058, -2.7566,  1.0539],\n",
      "        [ 3.9238,  0.2023,  2.4916,  ...,  0.8646,  1.0868,  1.9204],\n",
      "        [ 2.4435,  2.1998,  2.6546,  ...,  1.6592,  0.4002, -1.6802]])\n"
     ]
    }
   ],
   "source": [
    "# Z COMES FROM NORMAL(0, 1)\n",
    "num_preds = 100\n",
    "rand_v = torch.rand((num_preds, 32))\n",
    "#p = torch.distributions.Normal(torch.zeros_like(rand_v), torch.zeros_like(rand_v))\n",
    "#z = p.rsample()\n",
    "latent_dim = 32\n",
    "z = np.random.normal(size=(num_preds, latent_dim))\n",
    "z = torch.FloatTensor(z)\n",
    "with torch.no_grad():\n",
    "    pred = vae.decoder(z.to(device)).cpu()\n",
    "    print(pred)\n",
    "\n",
    "def inverseScaler(xscaled, xmin, xmax, min, max):\n",
    "    scale = (max - min) / (xmax - xmin)\n",
    "    xInv = (xscaled/scale) - (min/scale) + xmin\n",
    "    return xInv\n",
    "#print(y_pred)\n",
    "pred = inverseScaler(pred, min_input, max_input, 0, 1)\n",
    "# SAMPLE IMAGES\n",
    "#with torch.no_grad():\n",
    "#    pred = pl_module.decoder(z).cpu()\n",
    "print(pred)\n",
    "np.save(directory + 'y_pred', pred)\n",
    "#!scp -r y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "y_pred = pred\n",
    "U_gen = y_pred[:, :851101*3]\n",
    "nut_gen = y_pred[:, 851101*3:851101*4]\n",
    "Unut_data = np.load(directory + input_filename)\n",
    "\n",
    "\n",
    "U_mag_gen = np.sqrt(U_gen[:, :851101]**2 + U_gen[:, 851101:851101*2]**2 + U_gen[:, 851101*2:851101*3]**2)\n",
    "df1 = pd.DataFrame(U_mag_gen)\n",
    "df1['label'] = 'U'\n",
    "df2 = pd.DataFrame(nut_gen)\n",
    "df2['label'] = 'nut'\n",
    "\n",
    "df1 = df1.append(df2, ignore_index=True)\n",
    "label = df1[\"label\"]\n",
    "df1 = df1.drop(labels = [\"label\"],axis = 1)\n",
    "#TSNE\n",
    "\n",
    "Unut_gen_embedded = TSNE(n_components=2).fit_transform(df1)\n",
    "tsne2done = Unut_gen_embedded[:, 0]\n",
    "tsne2dtwo = Unut_gen_embedded[:, 1]\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=tsne2done, y=tsne2dtwo,\n",
    "    hue=label,\n",
    "    data=df1,\n",
    "    alpha=0.5,\n",
    "    markers='s'\n",
    ")\n",
    "\n",
    "\n",
    "U_mag_real = np.sqrt(Unut_data[:, :851101]**2 + Unut_data[:, 851101:851101*2]**2 + Unut_data[:, 851101*2:851101*3]**2)\n",
    "df1 = pd.DataFrame(U_mag_real)\n",
    "df1['label'] = 'U'\n",
    "df2 = pd.DataFrame(Unut_data[:, 851101*3:851101*4])\n",
    "df2['label'] = 'nut'\n",
    "\n",
    "df1 = df1.append(df2, ignore_index=True)\n",
    "label = df1[\"label\"]\n",
    "df1 = df1.drop(labels = [\"label\"],axis = 1)\n",
    "\n",
    "Unut_gen_embedded = TSNE(n_components=2).fit_transform(df1)\n",
    "tsne2done = Unut_gen_embedded[:, 0]\n",
    "tsne2dtwo = Unut_gen_embedded[:, 1]\n",
    "sns.scatterplot(\n",
    "    x=tsne2done, y=tsne2dtwo,\n",
    "    hue=label,\n",
    "    data=df1,\n",
    "    alpha=1,\n",
    "    markers='s'\n",
    ")\n",
    "\n",
    "df1 = pd.DataFrame(U_mag_real)\n",
    "df1['label'] = 'U_real'\n",
    "df2 = pd.DataFrame(Unut_data[:, 851101*3:851101*4])\n",
    "df2['label'] = 'nut_real'\n",
    "df1 = df1.append(df2, ignore_index=True)\n",
    "\n",
    "df3 = pd.DataFrame(U_mag_gen)\n",
    "df3['label'] = 'U_gen'\n",
    "df1 = df1.append(df3, ignore_index=True)\n",
    "\n",
    "df4 = pd.DataFrame(nut_gen)\n",
    "df4['label'] = 'nut_gen'\n",
    "df1 = df1.append(df4, ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fbe108be85f43cbd1186526061e7051cd9997bf100f68fec91c85a6a079be9f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
